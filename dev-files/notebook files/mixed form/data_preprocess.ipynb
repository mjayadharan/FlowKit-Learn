{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data to feed into the FlowNet class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currently take in rectangular boundayr: could improve on it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocess:\n",
    "    \"\"\"\n",
    "    Class to preprocess data before going into the FlowNet class. This is used as part of the \n",
    "    FlowNet module, but can also be used as a standalone data preprocessor. Only depends on numpy\n",
    "    from external packages. Can deal with any iterable numpy-type arrays (including python lists,\n",
    "    numpy nd-arrays, tf-tensors tc), which are inturn converted to numpy nd-arrays after processing.\n",
    "    Can further improve on this by developing a more efficient and general data processing pipeline \n",
    "    using features from either pandas or TensorFlow.\n",
    "    Currently doesn't do any scaling, but scaling feature can easily be implemented using keras api \n",
    "    scaling wrapper. \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    def __init__(self, space_dim, dom_bounds=[[]], time_dep=False):\n",
    "        \"\"\"\n",
    "        space_dim (int) - dimension of the space.\n",
    "        \n",
    "        dom_bounds (list of lists) - list of space_dim number of elements,\n",
    "        where each element is an intervel giving bound on the space domain,\n",
    "        dom_bounds[-1] is time boudns if time_dep=True.\n",
    "        \n",
    "        time_dep (bool) - true if the pde is time dependent.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.space_dim = space_dim\n",
    "        self.dom_bounds = dom_bounds\n",
    "        self.time_dep = time_dep\n",
    "        self.problem_dim = space_dim + time_dep\n",
    "        if dom_bounds[0]:\n",
    "            assert (len(dom_bounds) == self.problem_dim), \"domain bounds given incompatible with the space-time dimension\"\n",
    "     \n",
    "    def prepare_input_data(self, X_data, Y_data=[]):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "        ---------\n",
    "        X_data (numpy type iterable) - array of shape (m,p) where m is the number of data points \n",
    "        and p is the number features (space_dim of domain of pde). This could contain both boundary\n",
    "        and initial condition data for the pde. \n",
    "        Y_data (numpy type iterable) - array of shape (m,o) where m is the number of data points \n",
    "        and o is the dimension of the output. o is autmatically determined from column size of Y_data.\n",
    "        Y_data is empty if X_data is meant to make predictions.\n",
    "        \n",
    "        return: X_data_np, Y_data_np.\n",
    "        ------\n",
    "        X_data_np (list of numpy arrays) - numpy array of X_data.\n",
    "        Y_data_np (numpy type iterable) - Returned only if Y_data is non-empty. Array of shape (m,o+1).\n",
    "        where m is the number of data points and o is the dimension of the output. \n",
    "        \n",
    "        Purpose: Prepares the data to make it compatible to be fed to FlowNet's predict, evaluate, train etc.\n",
    "        If Y_data is empty, then X_data is just treated as points for prediction. Otherwise Y_data is also \n",
    "        processed, either for feeding to self.get_training_data or to feed to FlowNet's evaluate method. \n",
    "        \"\"\"\n",
    "        \n",
    "        #asserting that X_data and problem dimensions are compatible\n",
    "        assert (len(X_data[0]) == self.space_dim + self.time_dep), \"X_data and problem dimensions incompatible\"\n",
    "        X_data_np = np.array(X_data)\n",
    "        X_data_return = [X_data_np[:,i,np.newaxis] for i in range(len(X_data_np[0]))]\n",
    "        if len(Y_data) > 0: \n",
    "            #asserting that X_data and Y_data are compatible\n",
    "            assert (len(X_data) == len(Y_data)), \"X_data and Y_data incompatible, make sure they have the same number of data points\"\n",
    "            Y_data_np = np.array(Y_data)\n",
    "            #Concatenating an array of 1 to the right of Y_data to show that it is part of actual data (not pde collocation points)\n",
    "            Y_data_np = np.concatenate([Y_data, np.ones((len(Y_data),1))], axis=1)\n",
    "            return X_data_return, Y_data_np\n",
    "        else:\n",
    "            return X_data_return\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def get_training_data(self, X_data, Y_data, X_col_points=[], dist=\"uniform\", shuffle=False):\n",
    "        \"\"\"\n",
    "        Prepares the input data which will be later combined with pde collocation points.\n",
    "        \n",
    "        arguments: \n",
    "        ----------\n",
    "        X_data (numpy type iterable) - array of shape (m,p) where m is the number of data points \n",
    "        and p is the number features (space_dim of domain of pde). This could contain both boundary\n",
    "        and initial condition data for the pde. \n",
    "        \n",
    "        Y_data (numpy type iterable) - array of shape (m,o) where m is the number of data points \n",
    "        and o is the dimension of the output. o is autmatically determined from column size of Y_data.\n",
    "        \n",
    "        X_col_points (numpy type iterable or int) - If a number is given, then that  number of collocation\n",
    "        points will be  generated by randomly selecting poins from insdie the domain.\n",
    "        If this is given to be an iterable of type X_data, these points will be used as the collocation points\n",
    "        instead of generating new collocation points.\n",
    ".\n",
    "        dist (string key) - distribution of the collocation points inside the doman. Currently options \n",
    "        include \"uniform\" and \"normal\"\n",
    "        \n",
    "        shuffle (bool) - whether the data needs to be shuffled before feeding into FlowNet\n",
    "        \n",
    "        return: X_train, Y_train\n",
    "        ------------------------\n",
    "        X_train (list of numpy arrays) - list of p numpy array of shape (m,1).\n",
    "        \n",
    "        Y_train (numpy type iterable) - array of shape (m,o+1) where m is the number of data points \n",
    "        and o is the dimension of the output.\n",
    "        \"\"\"\n",
    "        \n",
    "        possible_dist = {\"uniform\", \"normal\"}\n",
    "        assert (dist in possible_dist), \"given distribution for collocation points is not supported\"\n",
    "        \n",
    "        if type(X_col_points) == int:\n",
    "            num_col_points = X_col_points\n",
    "            if dist == \"uniform\":\n",
    "                X_col_points = [np.random.uniform(self.dom_bounds[i][0],\n",
    "                                                     self.dom_bounds[i][1],\n",
    "                                                     num_col_points).reshape(num_col_points,1)\n",
    "                                   for i in range(self.problem_dim)\n",
    "\n",
    "                                  ]\n",
    "            elif dist == \"normal\":\n",
    "                X_col_points = [np.random.normal(self.dom_bounds[i][0],\n",
    "                                                     self.dom_bounds[i][1],\n",
    "                                                     num_col_points).reshape(num_col_points,1)\n",
    "                                   for i in range(self.problem_dim)\n",
    "\n",
    "                                  ]\n",
    "        else:\n",
    "            X_col_points = self.prepare_input_data(X_col_points)\n",
    "            \n",
    "        #Generating the zero right hand side for the pde Pde(x,t) = 0, right column is 0.0 to indicate\n",
    "        #that the a pde loss function is to be used for training. refer to FlowNet::pde_loss\n",
    "        Y_col_points = np.zeros( (len(X_col_points[0]), len(Y_data[0])+1) )\n",
    "\n",
    "\n",
    "        X_bc_ic, Y_bc_ic =  self.prepare_input_data(X_data, Y_data)\n",
    "        \n",
    "        #In the format compatible for FlowNet\n",
    "        assert (len(X_bc_ic) == len(X_col_points)), \"Generated X_col_points and X_bc_ic condition points are with incompatible size\"\n",
    "        X_tr_combined = [np.concatenate( [X_bc_ic[i], X_col_points[i]], axis=0)  for i in range(len(X_bc_ic)) ]\n",
    "        Y_tr_combined = np.concatenate([Y_bc_ic, Y_col_points])\n",
    "\n",
    "        return X_tr_combined, Y_tr_combined\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Data_preprocess(2, [[-1,1],[-1,1],[0,1]], time_dep=True)\n",
    "X = [[1,2,3],[1,2,3], [3,5,6], [9,3,4],[2,4,5]]\n",
    "Y = [[1],[2],[3],[4],[5]]\n",
    "# d.prepare_input_data(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if a[0]:\n",
    "    print(\"something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "       array([11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
       "       array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])], dtype=object)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([np.arange(1,10), np.arange(11,20), np.arange(30,40)])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "a = tf.constant([[1,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [2., 1.],\n",
       "       [3., 1.],\n",
       "       [4., 1.],\n",
       "       [5., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [2., 1.],\n",
       "       [3., 1.],\n",
       "       [4., 1.],\n",
       "       [5., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.00000000e+00],\n",
       "        [ 1.00000000e+00],\n",
       "        [ 3.00000000e+00],\n",
       "        [ 9.00000000e+00],\n",
       "        [ 2.00000000e+00],\n",
       "        [-1.95423007e-03],\n",
       "        [-8.52798111e-01],\n",
       "        [ 9.57757391e-01],\n",
       "        [ 3.85120506e-01],\n",
       "        [ 8.34758850e-02],\n",
       "        [-2.01348642e-01],\n",
       "        [-7.32595451e-01]]),\n",
       " array([[ 2.        ],\n",
       "        [ 2.        ],\n",
       "        [ 5.        ],\n",
       "        [ 3.        ],\n",
       "        [ 4.        ],\n",
       "        [ 0.91013501],\n",
       "        [-0.0828337 ],\n",
       "        [-0.31452133],\n",
       "        [ 0.44855684],\n",
       "        [ 0.71382833],\n",
       "        [ 0.5872033 ],\n",
       "        [ 0.50673602]]),\n",
       " array([[3.        ],\n",
       "        [3.        ],\n",
       "        [6.        ],\n",
       "        [4.        ],\n",
       "        [5.        ],\n",
       "        [0.94060961],\n",
       "        [0.33172665],\n",
       "        [0.92466052],\n",
       "        [0.88582875],\n",
       "        [0.98290645],\n",
       "        [0.97086624],\n",
       "        [0.62076529]])]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_tr = d.get_training_data(X,Y,X_col_points=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {\"11\", \"something\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
