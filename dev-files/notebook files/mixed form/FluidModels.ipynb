{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module containng custom Keras models and layers required for FlowNet architecture.\n",
    "\"\"\"\n",
    "\n",
    "try:     \n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import backend as K\n",
    "except Exception as e:\n",
    "        raise Exception(\"Error occured while importing dependency packages. More details:\\n\",e)\n",
    "        \n",
    "__author__ = \"Manu Jayadharan\"\n",
    "__copyright__ = \"Copyright 2020, FlowNet\"\n",
    "__credits__ = [\"Manu Jayadharan\"]\n",
    "__license__ = \"\"\n",
    "__version__ = \"0.1.0\"\n",
    "__maintainer__ = \"Manu Jayadharan\"\n",
    "__email__ = \"manu.jayadharan@pitt.edu\"\n",
    "__status__ = \"Development\"     \n",
    "\n",
    "class ForwardModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Model to construct FNN (Forward Neural Network) using custom Keras layers. Subclass of tf.keras.Model\n",
    "    \"\"\"\n",
    "      \n",
    "    def __init__(self, space_dim=1, time_dep=False, output_dim=1,\n",
    "                 n_hid_lay=3, n_hid_nrn=20, act_func = \"tanh\", rhs_func = None):\n",
    "        \"\"\"\n",
    "        space_dim (int) -> Dimension of the space Omega where the PDE is defined.\n",
    "        time_dep (bool) -> True if the problem is time dependent.\n",
    "        output_dim (int) -> Dimension of the range of the solution to PDE.\n",
    "        \n",
    "        n_hid_layer (int) -> Number of hidden layers in the neural network.\n",
    "        n_hid_nrn (int) -> Number of neurons in each hidden layer of the NN.\n",
    "        \n",
    "        act_func (string) -> Activation functions for each of the hidden layers. Has to\n",
    "                            be one of the members of keras.activations: could be one of\n",
    "                            {\"tanh\", \"sigmoid\", \"elu\", \"relu\", \"exponential\"}\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ForwardModel, self).__init__()\n",
    "        \n",
    "        #Defining class atributes\n",
    "        self.space_dim = space_dim\n",
    "        self.time_dep = time_dep\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hid_lay = n_hid_lay\n",
    "        self.n_hid_nrn = n_hid_nrn\n",
    "        \n",
    "        #Block of hidden layers\n",
    "        self.hidden_block = [keras.layers.Dense( self.n_hid_nrn, activation=act_func,\n",
    "                                           name=\"dense_\"+str(i+1) ) for i in range(n_hid_lay)]\n",
    "        #Final output layer\n",
    "        self.final_layer = keras.layers.Dense(self.output_dim,\n",
    "                                         name=\"final_layer\")        \n",
    "        \n",
    "        #Defining the rhs of PDE: P(u,delu) = f(x,t)\n",
    "        if rhs_func != None:\n",
    "            self.rhs_function = rhs_func\n",
    "        else:\n",
    "            self.rhs_function = lambda x: 0\n",
    "        \n",
    "    def findGrad(self,func,input_space):\n",
    "        \"\"\"\n",
    "        Find gradient with respect to the domain Omega of the PDE. \n",
    "        (tensor, tensor) -> Keras.Lambda layer\n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        func (tf tensor): function represented by tf tensor structure (Usually of size:\n",
    "              data_size x dim_output_previous_layer). The func is usually the final output (solution u)\n",
    "              coming out of a hidden layer\n",
    "        \n",
    "        input_space: argument with respect to which we need the partial derrivatives of func. Usually a list of \n",
    "              input arguments representing the space dimension.\n",
    "        \n",
    "        Output: Keras.Lambda layer. Note that output of such a lambda layer will be a list of tensors with\n",
    "                each element giving partial derrivative wrt to each element in argm.\n",
    "        \n",
    "        See tf.Keras.Lambda and tf.gradients for more details.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return keras.layers.Lambda(lambda z: [tf.gradients(z[0],x_i,\n",
    "                                                               unconnected_gradients='zero')\n",
    "                                                  for x_i in z[1] ]) ([func, input_space])\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in finding the time derrivative  lambda layer of type {} as follows: \\n{}\".format(type(e)),e)\n",
    "          \n",
    "        \n",
    "    def findTimeDer(self,func,input_time):\n",
    "        \"\"\"\n",
    "        (tensor, tensor) -> Keras.Lambda layer\n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        func (tf tensor): function represented by tf tensor structure (Usually of size:\n",
    "              data_size x dim_output_previous_layer). The func is usually the final output (solution u)\n",
    "              coming out of a hidden layer\n",
    "        \n",
    "        input_time: TensorFlow tensor. This should be the element of the input list which corresponds to the time\n",
    "              dimension. Used only if the problem is time_dependent.\n",
    "        \n",
    "        Output: Keras.Lambda layer. Note that output of such a lambda layer will be a tensor of size m x 1 \n",
    "                representing the time derrivative of output func.\n",
    "        \n",
    "        \n",
    "        See tf.Keras.Lambda and tf.gradients for more details.\n",
    "        \n",
    "        \"\"\"\n",
    "        assert (self.time_dep), \"Tried taking time derrivative even though the problem is not time dependent.\"\n",
    "        try:\n",
    "            return keras.layers.Lambda(lambda z: tf.gradients(z[0],z[1],\n",
    "                                                               unconnected_gradients='zero') [0]) ([func, input_time])\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in find gradient lambda layer of type {} as follows: \\n{} \".format(type(e)),e)\n",
    "            \n",
    "            \n",
    "    def findLaplace(self,first_der,input_space):\n",
    "        \"\"\"\n",
    "        (tensor, tensor) -> Keras.Lambda layer\n",
    "        \n",
    "        Returns lambda layer to find the laplacian of the solution to pde. \n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        first_der (tf tensor): function represented by tf tensor structure (Usually of size:\n",
    "              data_size x dim_output_previous_layer). The func is \n",
    "        \n",
    "        input_space: argument with respect to which we need the partial derrivatives of func. Usually a list of \n",
    "                     input arguments representing the space dimension.\n",
    "        \n",
    "        Output: Keras.Lambda layer. This lambda layer outputs the laplacian of solution function u.  \n",
    "        \n",
    "        See tf.Keras.Lambda and tf.gradients for more details.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # list containng diagonal entries of hessian matrix. Note that  tf.gradients \n",
    "            #returns a list of tensors and hence thats why we have  a [0] at the end of  \n",
    "            #the tf.gradients fucntion as tf.gradients(func,argm) [0]\n",
    "            del_sq_layer = keras.layers.Lambda( lambda z: [ tf.gradients(z[0][i], z[1][i],\n",
    "                                                              unconnected_gradients='zero') [0]\n",
    "                                                  for i in range(len(z[1])) ] ) ([first_der,input_space])\n",
    "            return sum(del_sq_layer)\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in find laplacian lambda layer of type {} as follows: \\n{}\".format(type(e)),e)\n",
    "    \n",
    "    #final layer representing the lhs P(x,t) of PDE P(x,t)=0\n",
    "    def findPdeLayer(self, laplacian, input_arg, time_der=0):\n",
    "        \"\"\"\n",
    "        (tensor, tensor, tensor) -> Keras.Lambda layer\n",
    "        \n",
    "        Returns lambda layer to find the actual pde P(u,delu,x,t) such that P(u,delu,x,t)=0. \n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        laplacian (tf tensor): laplacian with respect to space dim .\n",
    "        \n",
    "        input_arg: list of inputs corresponding to both space and time dimension. Last elemetn of \n",
    "                   the list corresponds to the temporal dimension.\n",
    "        \n",
    "        Output: Keras.Lambda layer. This lambda layer outputs the PDE P(u,delu, x,t).  \n",
    "        \n",
    "        See tf.Keras.Lambda and tf.gradients for more details.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "#             return keras.layers.Lambda(lambda z: z[0] - z[1] - tf.sin(z[2][0]+z[2][1]) - \n",
    "#                                        2*z[2][2]*tf.sin(z[2][0]+z[2][1])) ([time_der, laplacian, input_arg])\n",
    "            return keras.layers.Lambda(lambda z: z[0] - z[1] - self.rhs_function(input_arg)) ([time_der, laplacian, input_arg])\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in finding pde  lambda layer of type {} as follows: \\n{}\".format(type(e)),e)\n",
    "    \n",
    "    def get_config(self):\n",
    "        #getting basic config using the parent model class\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"space_dim\": self.space_dim, \n",
    "                \"time_dep\": self.time_dep, \"output_dim\": self.output_dim,\n",
    "                 \"n_hid_lay\": self.n_hid_lay, \"n_hid_nrn\": self.n_hid_nrn,\n",
    "                \"act_func\": self.act_func }\n",
    "    \n",
    "    def from_config(self, config, custom_objects):\n",
    "        super().from_config(config)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Call function which wll be used while training, prediciton and evaluation of the ForwardModel. \n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        inputs (list of tensors) -> last element of the list corresponds to temporal diimension if \n",
    "                                    self.time_dep = True. If possible, always feed the data from the \n",
    "                                    data processing method in flowDataProcess module.\n",
    "        training (bool) -> True if calling the function for training. False for prediction and evaluation. \n",
    "                           Value of triainng will be automatically taken care of by Keras. \n",
    "        \n",
    "        Note that inputs should always be given as a list with the last element of the list representing the \n",
    "        dimension corresponding to time.\n",
    "        \"\"\"\n",
    "        if self.time_dep:\n",
    "            try:\n",
    "                assert(len(inputs) > 1)\n",
    "                input_space = inputs[:-1]\n",
    "                input_time = inputs[-1]\n",
    "            except Exception as e:\n",
    "                raise Exception(\"Error occured while separating spacial and temporal data from inputs,\\\n",
    "                make sure that spacio-temporal data is being used to for training and \\\n",
    "                x=[space_dim1,..,space_dimn,time_dim]. More details on error below:\\n\", type(e), e)\n",
    "        else:\n",
    "            input_space = inputs\n",
    "        \n",
    "        #concatening all the input data (space and time dimensions) making it \n",
    "        #read to be passed to the hidden layers\n",
    "        hidden_output = keras.layers.concatenate(inputs) \n",
    "        \n",
    "        #hidden layers\n",
    "        for layer_id in range(self.n_hid_lay):\n",
    "            hidden_output = self.hidden_block[layer_id] (hidden_output)\n",
    "\n",
    "        \n",
    "        #output layer, this is typically the solution function\n",
    "        output_layer = self.final_layer(hidden_output)\n",
    "        \n",
    "        if training:\n",
    "            #pde specific layers\n",
    "            grad_layer = self.findGrad(output_layer, input_space)\n",
    "            laplace_layer = self.findLaplace(grad_layer, input_space)\n",
    "            if self.time_dep: \n",
    "                time_der_layer = self.findTimeDer(output_layer, input_time)\n",
    "            else:\n",
    "                time_der_layer=0\n",
    "            pde_layer = self.findPdeLayer(laplace_layer, inputs, time_der_layer)\n",
    "\n",
    "            return output_layer, pde_layer\n",
    "        \n",
    "        elif not training: #only outputting the function value if not tranining.\n",
    "                return output_layer\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poission(ForwardModel):\n",
    "    \"\"\"\n",
    "    Doc string goes here\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, space_dim=1, perm_tensor=None, output_dim=1,\n",
    "                 n_hid_lay=3, n_hid_nrn=20, act_func = \"tanh\", rhs_func = None):\n",
    "        \"\"\"\n",
    "        talk about super initialization\n",
    "        \"\"\"\n",
    "        \n",
    "        super.__init__(space_dim=space_dim, time_dep=False, output_dim=output_dim,\n",
    "                 n_hid_lay=n_hid_lay, n_hid_nrn=n_hid_nrn, act_func = act_func, rhs_func = rhs_func)\n",
    "        \n",
    "        self._perm_tensor = perm_tensor if perm_tensor else tf.eye(space_dim)\n",
    "\n",
    "    #final layer representing the lhs P(x) of PDE P(x)=0\n",
    "    def findPdeLayer(self, laplacian, input_arg):\n",
    "        \"\"\"\n",
    "        (tensor, tensor, tensor) -> Keras.Lambda layer\n",
    "        \n",
    "        Returns lambda layer to find the actual pde P(u,delu,x,t) such that P(u,delu,x,t)=0. \n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        laplacian (tf tensor): laplacian with respect to space dim .\n",
    "        \n",
    "        input_arg: list of inputs corresponding to both space and time dimension. Last elemetn of \n",
    "                   the list corresponds to the temporal dimension.\n",
    "        \n",
    "        Output: Keras.Lambda layer. This lambda layer outputs the PDE P(u,delu, x,t).  \n",
    "        \n",
    "        See tf.Keras.Lambda and tf.gradients for more details.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return keras.layers.Lambda(lambda z: -z[0] - self.rhs_function(input_arg)) ([laplacian, input_arg])\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in finding pde  lambda layer of type {} as follows: \\n{}\".format(type(e)),e)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Call function which wll be used while training, prediciton and evaluation of the ForwardModel. \n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        inputs (list of tensors) -> last element of the list corresponds to temporal diimension if \n",
    "                                    self.time_dep = True. If possible, always feed the data from the \n",
    "                                    data processing method in flowDataProcess module.\n",
    "        training (bool) -> True if calling the function for training. False for prediction and evaluation. \n",
    "                           Value of triainng will be automatically taken care of by Keras. \n",
    "        \n",
    "        Note that inputs should always be given as a list with the last element of the list representing the \n",
    "        dimension corresponding to time.\n",
    "        \"\"\"\n",
    "        if self.time_dep:\n",
    "            input_space = inputs\n",
    "            \n",
    "        #concatening all the input data (space and time dimensions) making it \n",
    "        #read to be passed to the hidden layers\n",
    "        hidden_output = keras.layers.concatenate(inputs) \n",
    "        \n",
    "        #hidden layers\n",
    "        for layer_id in range(self.n_hid_lay):\n",
    "            hidden_output = self.hidden_block[layer_id] (hidden_output)\n",
    "\n",
    "        \n",
    "        #output layer, this is typically the solution function\n",
    "        output_layer = self.final_layer(hidden_output)\n",
    "        \n",
    "        if training:\n",
    "            #pde specific layers\n",
    "            grad_layer = self.findGrad(output_layer, input_space)\n",
    "            laplace_layer = self.findLaplace(grad_layer, input_space)\n",
    "            pde_layer = self.findPdeLayer(laplace_layer, inputs)\n",
    "\n",
    "            return output_layer, pde_layer\n",
    "        \n",
    "        elif not training: #only outputting the function value if not tranining.\n",
    "                return output_layer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedDif(ForwardModel):\n",
    "    \"\"\"\n",
    "    Doc string goes here\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, space_dim=1, perm_tensor=None, output_dim=1,\n",
    "                 n_hid_lay=3, n_hid_nrn=20, act_func = \"tanh\", rhs_func = None,\n",
    "                n_xtr_hid_lay=1):\n",
    "        \"\"\"\n",
    "        talk about super initialization\n",
    "        \"\"\"\n",
    "        \n",
    "        super.__init__(space_dim=space_dim, time_dep=True, output_dim=output_dim,\n",
    "                 n_hid_lay=n_hid_lay, n_hid_nrn=n_hid_nrn, act_func = act_func, rhs_func = rhs_func)\n",
    "        \n",
    "        self._perm_tensor = perm_tensor if perm_tensor else tf.eye(space_dim)\n",
    "        self.n_xtr_hid_lay = n_xtr_hid_lay\n",
    "        \n",
    "        self.__pressure_dim = 1\n",
    "        # Defining final output layers for pressure and velocity\n",
    "        #pressure\n",
    "        self.__pres_final_layer = keras.layers.Dense(self.__pressure_dim,\n",
    "                                         name=\"pressure_out_layer\")\n",
    "        self.__vel_final_layer = keras.layers.Dense(space_dim, name = \"velocity_out_layer\")\n",
    "        \n",
    "        #Block of extra hidden layers for velocity\n",
    "        self.xtra_hidden_block = [keras.layers.Dense( self.n_hid_nrn, activation=act_func,\n",
    "                                           name=\"dense_xtra_\"+str(i+1) ) for i in range(n_xtr_hid_lay)]\n",
    "        \n",
    "    def findDivLayer(self,vel_vector,input_space):\n",
    "        \"\"\"\n",
    "        (tensor, tensor) -> Keras.Lambda layer\n",
    "        \n",
    "        Returns lambda layer to find the divergence of the velocity vector. \n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        vel_vector (tf tensor): predicted velocity function represented by tf tensor structure (Usually of size:\n",
    "              data_size x space_dim). \n",
    "        \n",
    "        input_space: argument with respect to which we need the partial derrivatives of func. Usually a list of \n",
    "                     input arguments representing the space dimension.\n",
    "        \n",
    "        Output: Keras.Lambda layer. This lambda layer outputs the laplacian of solution function u.  \n",
    "        \n",
    "        See tf.Keras.Lambda and tf.gradients for more details.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # list containng diagonal entries of the gradient \n",
    "            #on velocity vector. Note that  tf.gradients \n",
    "            #returns a list of tensors and hence thats why we have  a [0] at the end of  \n",
    "            #the tf.gradients fucntion as tf.gradients(func,argm) [0]\n",
    "            grad_diag= keras.layers.Lambda( lambda z: [ tf.gradients(z[0][:,i], z[1][i],\n",
    "                                                              unconnected_gradients='zero') [0]\n",
    "                                                  for i in range(len(z[1])) ] ) ([vel_vector,input_space])\n",
    "            return sum(grad_diag)\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in finding divergence of velocity lambda layer of type {} as follows: \\n{}\".format(type(e)),e)\n",
    "\n",
    "    #layer representing the scalar equation in the pde, PDE P_scalar(x,t)=0.\n",
    "    #This is typically something of the form p_t - div(u) -f =0\n",
    "    def findPdeLayer_1(self, div_vel, input_arg, time_der=0):\n",
    "        \"\"\"\n",
    "        (tensor, tensor, tensor) -> Keras.Lambda layer\n",
    "        \n",
    "        Returns lambda layer to find the actual pde P(u,delu,x,t) such that P(u,delu,x,t)=0. \n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        div_vel(tf tensor): divergence of velocity vector with respect to space dim .\n",
    "        \n",
    "        input_arg: list of inputs corresponding to both space and time dimension. Last elemetn of \n",
    "                   the list corresponds to the temporal dimension.\n",
    "        \n",
    "        time_der (tf tensor): derrivative of the pressure function with respect to time.\n",
    "        \n",
    "        Output: Keras.Lambda layer. This lambda layer outputs the PDE P(u,delu, x,t).  \n",
    "        \n",
    "        See tf.Keras.Lambda and tf.gradients for more details.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "#             return keras.layers.Lambda(lambda z: z[0] - z[1] - tf.sin(z[2][0]+z[2][1]) - \n",
    "#                                        2*z[2][2]*tf.sin(z[2][0]+z[2][1])) ([time_der, laplacian, input_arg])\n",
    "            return keras.layers.Lambda(lambda z: z[0] - z[1] - self.rhs_function(input_arg)) ([time_der, div_vel, input_arg])\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in finding pressure pde  lambda layer of type {} as follows: \\n{}\".format(type(e)),e)\n",
    "     \n",
    "    \n",
    "    #layer representing the vector equation in the pde, PDE PDE_vector(x,t)=0.\n",
    "    #This is typically something of the form grad(p) + u = 0 \n",
    "    def findPdeLayer_2(self, grad_p, vel_vect):\n",
    "        \"\"\"\n",
    "        (tensor, tensor) -> Keras.Lambda layer\n",
    "        \n",
    "        Returns list of lambda layer to find the vector PDE= 0 in the mixed form.\n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        grad_p(list of tf tensors): gradient of p with respect so spacedim. each element of the list\n",
    "                    corresponds to the partial with respect to different variable in spacedim..\n",
    "        \n",
    "        vel_vect(tensor): velocity vector coming from teh final xtra hidden layer. This has shape,\n",
    "                            num_instances*vel_dim. note that vel_dim = space_dim.\n",
    "        \n",
    "        Output: list of  Keras.Lambda layer. This lambda layer outputs a list representing the vector\n",
    "                equation grad(p) + u =0.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return keras.layers.Lambda(lambda z: [z[0][i] + z[1][:,i] for i in range(len(grad_p))]) ([grad_p, vel_vect])\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in finding vector pde  lambda layer of type {} as follows: \\n{}\".format(type(e)),e)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Call function which wll be used while training, prediciton and evaluation of the ForwardModel. \n",
    "        \n",
    "        arguments:\n",
    "        ----------\n",
    "        inputs (list of tensors) -> last element of the list corresponds to temporal diimension if \n",
    "                                    self.time_dep = True. If possible, always feed the data from the \n",
    "                                    data processing method in flowDataProcess module.\n",
    "        training (bool) -> True if calling the function for training. False for prediction and evaluation. \n",
    "                           Value of triainng will be automatically taken care of by Keras. \n",
    "        \n",
    "        Note that inputs should always be given as a list with the last element of the list representing the \n",
    "        dimension corresponding to time.\n",
    "        \"\"\"\n",
    "        if self.time_dep:\n",
    "            try:\n",
    "                assert(len(inputs) > 1)\n",
    "                input_space = inputs[:-1]\n",
    "                input_time = inputs[-1]\n",
    "            except Exception as e:\n",
    "                raise Exception(\"Error occured while separating spacial and temporal data from inputs,\\\n",
    "                make sure that spacio-temporal data is being used to for training and \\\n",
    "                x=[space_dim1,..,space_dimn,time_dim]. More details on error below:\\n\", type(e), e)\n",
    "        else:\n",
    "            input_space = inputs\n",
    "        \n",
    "        #concatening all the input data (space and time dimensions) making it \n",
    "        #read to be passed to the hidden layers\n",
    "        hidden_output = keras.layers.concatenate(inputs) \n",
    "        \n",
    "        #hidden layers\n",
    "        for layer_id in range(self.n_hid_lay):\n",
    "            hidden_output = self.hidden_block[layer_id] (hidden_output)\n",
    "\n",
    "        \n",
    "#         #output layer, this is typically the solution function\n",
    "#         output_layer = self.final_layer(hidden_output)\n",
    "        \n",
    "        #pressure output layer\n",
    "        pres_output_layer = self.pres_final_layer(hidden_output)\n",
    "        \n",
    "        #velocity output layer\n",
    "        for layer_id in range(self.n_xtr_hid_lay):\n",
    "            hidden_output = self.xtra_hidden_block[layer_id] (hidden_output)\n",
    "        vel_output_layer = self.vel_final_layer(hidden_output)\n",
    "        \n",
    "        if training:\n",
    "            #pde specific layers\n",
    "            \n",
    "            #finding gradient of pressure with respect to space dim\n",
    "            grad_layer = self.findGrad(output_layer, input_space)\n",
    "#             laplace_layer = self.findLaplace(grad_layer, input_space)\n",
    "            #finding divergence of the velocity layer with respect to space dim\n",
    "            div_layer = self.findDivLayer(vel_output_layer, input_space)\n",
    "        \n",
    "            \n",
    "            if self.time_dep: \n",
    "                time_der_layer = self.findTimeDer(output_layer, input_time)\n",
    "            else:\n",
    "                time_der_layer=0\n",
    "            #PDE_1 = 0 the scalar equation of the form p_t - div(u) -f = 0\n",
    "            pde_layer_1 = self.findPdeLayer(div_layer, inputs, time_der_layer)\n",
    "            pde_layer_2 = self.findPdeLayer_2(grad_layer, vel_output_layer)\n",
    "\n",
    "            return output_layer, pde_layer_1, pde_layer_2\n",
    "        \n",
    "        elif not training: #only outputting the function value if not tranining.\n",
    "                return output_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
