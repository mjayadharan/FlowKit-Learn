{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module containing various data processing tools for FluidLearn PDE solver.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "__author__ = \"Manu Jayadharan\"\n",
    "__copyright__ = \"Copyright 2020, FluidLearn\"\n",
    "__credits__ = [\"Manu Jayadharan\"]\n",
    "__license__ = \"\"\n",
    "__version__ = \"0.1.0\"\n",
    "__maintainer__ = \"Manu Jayadharan\"\n",
    "__email__ = \"manu.jayadharan@pitt.edu\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "class Data_preprocess:\n",
    "    \"\"\"\n",
    "    Class to preprocess data before going into the FluidLearn class. This is used as part of the \n",
    "    FluidLearn module, but can also be used as a standalone data preprocessor. Only depends on numpy\n",
    "    from external packages. Can deal with any iterable numpy-type arrays (including python lists,\n",
    "    numpy nd-arrays, tf-tensors tc), which are inturn converted to numpy nd-arrays after processing.\n",
    "    Can further improve on this by developing a more efficient and general data processing pipeline \n",
    "    using features from either pandas or TensorFlow.\n",
    "    Currently doesn't do any scaling, but scaling feature can easily be implemented using keras api \n",
    "    scaling wrapper. \n",
    "    \"\"\"\n",
    "    def __init__(self, space_dim, dom_bounds=[[]], time_dep=False):\n",
    "        \"\"\"\n",
    "        space_dim (int) - dimension of the space.\n",
    "        \n",
    "        dom_bounds (list of lists) - list of space_dim number of elements,\n",
    "        where each element is an intervel giving bound on the space domain,\n",
    "        dom_bounds[-1] is time boudns if time_dep=True.\n",
    "        \n",
    "        time_dep (bool) - true if the pde is time dependent.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.space_dim = space_dim\n",
    "        self.dom_bounds = dom_bounds\n",
    "        self.time_dep = time_dep\n",
    "        self.problem_dim = space_dim + time_dep\n",
    "        if dom_bounds[0]:\n",
    "            assert (len(dom_bounds) == self.problem_dim), \"domain bounds given incompatible with the space-time dimension\"\n",
    "     \n",
    "    def prepare_input_data(self, X_data, Y_data=[]):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "        ---------\n",
    "        X_data (numpy type iterable) - array of shape (m,p) where m is the number of data points \n",
    "        and p is the number features (space_dim of domain of pde). This could contain both boundary\n",
    "        and initial condition data for the pde. \n",
    "        Y_data (numpy type iterable) - array of shape (m,o) where m is the number of data points \n",
    "        and o is the dimension of the output. o is autmatically determined from column size of Y_data.\n",
    "        Y_data is empty if X_data is meant to make predictions.\n",
    "        \n",
    "        return: X_data_np, Y_data_np.\n",
    "        ------\n",
    "        X_data_np (list of numpy arrays) - numpy array of X_data.\n",
    "        Y_data_np (numpy type iterable) - Returned only if Y_data is non-empty. Array of shape (m,o+1).\n",
    "        where m is the number of data points and o is the dimension of the output. \n",
    "        \n",
    "        Purpose: Prepares the data to make it compatible to be fed to FluidLearn's predict, evaluate, train etc.\n",
    "        If Y_data is empty, then X_data is just treated as points for prediction. Otherwise Y_data is also \n",
    "        processed, either for feeding to self.get_training_data or to feed to FluidLearn's evaluate method. \n",
    "        \"\"\"\n",
    "        \n",
    "        #asserting that X_data and problem dimensions are compatible\n",
    "        assert (len(X_data[0]) == self.space_dim + self.time_dep), \"X_data and problem dimensions incompatible\"\n",
    "        X_data_np = np.array(X_data)\n",
    "        X_data_return = [X_data_np[:,i,np.newaxis] for i in range(len(X_data_np[0]))]\n",
    "        if len(Y_data) > 0: \n",
    "            #asserting that X_data and Y_data are compatible\n",
    "            assert (len(X_data) == len(Y_data)), \"X_data and Y_data incompatible, make sure they have the same number of data points\"\n",
    "            Y_data_np = np.array(Y_data)\n",
    "            #Concatenating an array of 1 to the right of Y_data to show that it is part of actual data (not pde collocation points)\n",
    "            Y_data_np = np.concatenate([Y_data, np.ones((len(Y_data),1))], axis=1)\n",
    "            return X_data_return, Y_data_np\n",
    "        else:\n",
    "            return X_data_return\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def get_training_data(self, X_data, Y_data, X_col_points=[], dist=\"uniform\", shuffle=False):\n",
    "        \"\"\"\n",
    "        Prepares the input data which will be later combined with pde collocation points.\n",
    "        \n",
    "        arguments: \n",
    "        ----------\n",
    "        X_data (numpy type iterable) - array of shape (m,p) where m is the number of data points \n",
    "        and p is the number features (space_dim of domain of pde). This could contain both boundary\n",
    "        and initial condition data for the pde. \n",
    "        \n",
    "        Y_data (numpy type iterable) - array of shape (m,o) where m is the number of data points \n",
    "        and o is the dimension of the output. o is autmatically determined from column size of Y_data.\n",
    "        \n",
    "        X_col_points (numpy type iterable or int) - If a number is given, then that  number of collocation\n",
    "        points will be  generated by randomly selecting poins from insdie the domain.\n",
    "        If this is given to be an iterable of type X_data, these points will be used as the collocation points\n",
    "        instead of generating new collocation points.\n",
    ".\n",
    "        dist (string key) - distribution of the collocation points inside the doman. Currently options \n",
    "        include \"uniform\" and \"normal\"\n",
    "        \n",
    "        shuffle (bool) - whether the data needs to be shuffled before feeding into FluidLearn\n",
    "        \n",
    "        return: X_train, Y_train\n",
    "        ------------------------\n",
    "        X_train (list of numpy arrays) - list of p numpy array of shape (m,1).\n",
    "        \n",
    "        Y_train (numpy type iterable) - array of shape (m,o+1) where m is the number of data points \n",
    "        and o is the dimension of the output.\n",
    "        \"\"\"\n",
    "        \n",
    "        possible_dist = {\"uniform\", \"normal\"}\n",
    "        assert (dist in possible_dist), \"given distribution for collocation points is not supported\"\n",
    "        \n",
    "        if type(X_col_points) == int:\n",
    "            num_col_points = X_col_points\n",
    "            if dist == \"uniform\":\n",
    "                X_col_points = [np.random.uniform(self.dom_bounds[i][0],\n",
    "                                                     self.dom_bounds[i][1],\n",
    "                                                     num_col_points).reshape(num_col_points,1)\n",
    "                                   for i in range(self.problem_dim)\n",
    "\n",
    "                                  ]\n",
    "            elif dist == \"normal\":\n",
    "                X_col_points = [np.random.normal(self.dom_bounds[i][0],\n",
    "                                                     self.dom_bounds[i][1],\n",
    "                                                     num_col_points).reshape(num_col_points,1)\n",
    "                                   for i in range(self.problem_dim)\n",
    "\n",
    "                                  ]\n",
    "        else:\n",
    "            X_col_points = self.prepare_input_data(X_col_points)\n",
    "            \n",
    "        #Generating the zero right hand side for the pde Pde(x,t) = 0, right column is 0.0 to indicate\n",
    "        #that the a pde loss function is to be used for training. refer to FluidLearn::pde_loss\n",
    "        Y_col_points = np.zeros( (len(X_col_points[0]), len(Y_data[0])+1) )\n",
    "\n",
    "\n",
    "        X_bc_ic, Y_bc_ic =  self.prepare_input_data(X_data, Y_data)\n",
    "        \n",
    "        #In the format compatible for FluidLearn\n",
    "        assert (len(X_bc_ic) == len(X_col_points)), \"Generated X_col_points and X_bc_ic condition points are with incompatible size\"\n",
    "        X_tr_combined = [np.concatenate( [X_bc_ic[i], X_col_points[i]], axis=0)  for i in range(len(X_bc_ic)) ]\n",
    "        Y_tr_combined = np.concatenate([Y_bc_ic, Y_col_points])\n",
    "\n",
    "        return X_tr_combined, Y_tr_combined\n",
    " \n",
    "\n",
    "def save_to_csv(np_array, name_of_file=\"\"):\n",
    "    \"\"\"\n",
    "    (nd_array, string) - > None\n",
    "    Save the np_array in the csv format in the file mentioned in name_of_file. If the file name does \n",
    "    not include '.csv' at the end, it is added automatically.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert (len(np_array>0)), \"Error: tried to save incompatible file to csv format\"\n",
    "    name_of_file += \".csv\" if name_of_file[-4:] != \".csv\" else \"\"\n",
    "    \n",
    "    try:\n",
    "        np.savetxt(name_of_file, np_array, delimiter=',')\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Error occured while saving to csv file, of type {} as follows: \\n{}\".format(type(e),e))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def imp_from_csv(path_to_csv_file=\"\", x_y_combined=True, y_dim=1):\n",
    "    \"\"\"\n",
    "    (string, True, int) -> (numpy_nd_array_1, numpy_nd_array_2)\n",
    "    (string, False int) -> numpy_nd_array\n",
    "    \n",
    "    path_to_csv_file (string) : the address to the .csv file.\n",
    "    \n",
    "    x_y_combined (bool): if True returns a tuple with X_data in first component and \n",
    "    Y_data in second component, otherwise return a numpy_nd_array.\n",
    "    \n",
    "    y_dim (int) : dim of range of the predicted function.\n",
    "    \n",
    "    Loads data from a csv file and returns a numpy nd array. Compatible with save_to_csv.\n",
    "    make sure that path_to_csv_file refers to a .csv extension file.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert (path_to_csv_file[-4:] == \".csv\"), \"Error: incomptabile data given to path_to_csv_file\"\n",
    "\n",
    "    try:\n",
    "        loaded_array =  np.loadtxt(path_to_csv_file, delimiter=',')\n",
    "        if x_y_combined:\n",
    "            loaded_x = loaded_array[:,0:-y_dim]\n",
    "            loaded_y = loaded_array[:,-y_dim:]\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Error occured while loading csv file of  type {} as follows: \\n{}\".format(type(e),e))\n",
    "\n",
    "    if x_y_combined:\n",
    "        return loaded_x, loaded_y\n",
    "    else:\n",
    "        return loaded_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
